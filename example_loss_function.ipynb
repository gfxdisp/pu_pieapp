{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as pt\n",
    "from models.common import PerceptLossNet\n",
    "from loader.dataset import ImageLoader, image2patches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to reference and distorted iamges\n",
    "ref_path = './example_images/ldr/i06.bmp'\n",
    "dist_path ='./example_images/ldr/i06_15_5.bmp'\n",
    "dynamic_range = 'ldr'\n",
    "\n",
    "\n",
    "# Parameters of the display model (Assuming peak and black level of a display on which LDR image is shown).\n",
    "# Set to 100 and 0.5 if unsure. The parameter is not used for HDR images as these are given in luminance values.\n",
    "top_l = 100\n",
    "bot_l = 0.5\n",
    "\n",
    "# The quality assessment model operates on 64x64 patches sampled on a regular grid. \n",
    "# The shift specifies the window shift for sampling the patchs. The smaller the shift the more accurate the model is.\n",
    "stride = 16\n",
    "saved_state_model = '../net_only.pt'\n",
    "state = pt.load(saved_state_model, map_location='cpu')\n",
    "loader = ImageLoader()\n",
    "\n",
    "def read_convert_pt_image(image_path):\n",
    "    image = imageio.imread(image_path)\n",
    "    image = pt.from_numpy(imageio.core.asarray(image))\n",
    "    image = image.permute(2,0,1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run perceptual loss network (VGG style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network \n",
    "net = PerceptLossNet(state)\n",
    "# Switch off the training mode\n",
    "net.eval()\n",
    "\n",
    "# Read images and convert them into the pytorch tensorc with dimensions NCHW\n",
    "image_ref = read_convert_pt_image(ref_path)\n",
    "image_dis = read_convert_pt_image(dist_path)\n",
    "image_ref = image_ref.unsqueeze(0)\n",
    "image_dis = image_dis.unsqueeze(0)\n",
    "\n",
    "# Run the network without recorcding the gradient\n",
    "with pt.no_grad():\n",
    "    feature1r, feature2r, feature3r, feature4r, feature5r = net(image_ref,im_type=dynamic_range, lum_top=top_l, lum_bottom=bot_l)\n",
    "    feature1d, feature2d, feature3d, feature4d, feature5d = net(image_dis,im_type=dynamic_range, lum_top=top_l, lum_bottom=bot_l)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
